A general view of an installation by renowned street artist Banksy on October 2, 2019 in Croydon, England. The shop named 'Gross Domestic Product' appeared over night and features the stab vest he designed for Stormzy's headline act at the Glastonbury Festival, a Tony the Tiger rug and a cradle surrounded by CCTV cameras, at a disused retail outlet in Croydon. (Photo by Peter Summers/Getty Images) At a time when people have just returned from home, created temporary homes, or simply longed for home, it seems appropriate to reflect on the question: Can the digital future be our home? Shoshana Zuboff, in her book, The Age of Surveillance Capitalism: The fight for a human future at the new frontier of power, would argue that it is critical that we ask this question because how we approach it will determine the very possibility of a democratic society. As we define home, we define ourselves in relation to it. The notion of home has strong meanings and emotional associations. Home is a refuge, a sanctuary. It’s about safety and individuality. Home can also mean homelessness, gender violence, loss, economic inequality and devastation. Whether through presence or absence, home, in all its complexity, is important to us. Michael Fox, the author of Home: A Very Short Introduction, describes it as “a crucial point of reference – in memory, in feeling, and imagination – inventing the story of ourselves, our life-narrative, for understanding our place in time […] a vital link through which we connect with others and with the world and the universe at large”.  1 Socrates chose death by poison rather than exile from his beloved Athens and although his definition of home might have been complex, his exile equalled irrevocable and total loss. Gaston Bachelard, the French poet, philosopher and scientist whose name has become cultural shorthand for the link between memory and building, described the home as “the topography of our intimate being”. Our homes are worlds of their own, archives of memory, places of intimate conversation, sacred ritual and imagination.   1 In what way does the digital realm fit into our sense of home? Zuboff juxtaposes two possibilities of home in an effort to answer this question. The first, the “Aware Home”, was designed in 2000 by a group of scientists at Georgia Tech. The Aware Home was created as a living laboratory for the study of “ubiquitous computing” where animate and inanimate processes would be captured by a network of “context aware sensors” that are embedded in the house as well as in wearable computers worn by the home’s occupants. This design called for an “automated wireless collaboration” between the platform that hosted personal information for the occupants’ wearables and a second one that hosted the environmental information from the sensors. Zuboff explains: “There were three working assumptions: first, the scientists and engineers understood that the new data systems would produce an entirely new knowledge domain. Second, it was assumed that the rights to that new knowledge and the power to use it to improve one’s life would belong exclusively to the people who live in the house. Third, the team assumed that for all of its digital wizardry, the Aware Home would take its place as a modern incarnation of the ancient conventions that understand ‘home’ as the private sanctuary of those who dwell within its walls.” All of this was included in the engineering plan and the project emphasised trust, simplicity, the control and sovereignty of the occupants, and the inviolability of the home as a private domain. The information system was designed as a closed loop, meaning that there were only two nodes controlled entirely by the occupants. The house would trace the occupants’ whereabouts and activities as well as things such as medical conditions. As such, the team concluded that the occupants must have complete knowledge and control of the distribution of this information. Nineteen years later, and the global “smart home” market is valued at $36-billion annually and is expected to reach $151-billion within three years’ time. The second home that Zuboff highlights is one with a Nest-Thermostat – a smart-home device which was made by a company owned by the Google holding company, Alphabet. The Nest-Thermostat does many of the same things the Aware Home did. It collects data from other connected products – cars, ovens, fitness trackers, beds and so on; its systems can switch on the lights, signal video and audio recording, and send notifications to homeowners. But the knowledge produced by this system is not on a closed loop. It is uploaded to Google’s servers. Of course, the Nest-Thermostat comes with a privacy policy, terms of service agreement and an end-user license agreement. However, these policies and agreements mean that personal and sensitive household information is shared with myriad other smart devices, unnamed personnel, and third parties for the purposes of predictive analyses and sales to unspecified parties. A detailed analysis of Nest’s policies by University of London academics revealed that were one to enter into the Nest ecosystem of connected devices and apps, the purchase of a single thermostat would entail one to review nearly 1,000 contracts. Zuboff explains that the Aware Home was envisioned to empower the lives of the individual occupants. Each occupant had exclusive rights to the knowledge collected from the data. Today, the dream of the Aware Home is long gone and human experience is claimed as free raw material for translation into behavioural data. Although some of this data is used to improve a product or service, the rest is declared as free proprietary surplus. The right to sanctuary implicit in the creation of the Aware Home is now subjugated to the operation of “surveillance capitalism”, which Zuboff defines as, among other things, “an economic order that claims human experience as free raw material for hidden commercial practices of extraction, prediction and sales”. Data extraction for the purposes of prediction, and ultimately sales, is now the default model of most internet companies and although the notion of surveillance capitalism cannot be fully attributed to a single company (not even the Big Five – Google, Amazon, Facebook, Microsoft and Apple), it was most certainly invented and perfected by Google. Google’s initial stages and its dream “to organise the world’s information, making it universally accessible and useful”, echoed much of the same type of individual empowerment promised by the Aware Home. With its founders passionately opposed to advertising, the company promised information as a liberating and democratic social force. Google’s first revenues came from exclusive licensing deals to provide web services for Yahoo! and Japan’s BIGLOBE. It generated modest revenue from sponsored adverts linked to search query keywords so as to remain firmly objective in the delivery of its services. It was when the Dotcom bubble burst in 2000 that the company’s “absence of a sustainable market transaction”, spelled the beginnings of a 180-degree turn. For all the company’s innovation and democratic ideals, it could not turn its investors’ money into revenue. Changes had to be made in order to explore possible bigger revenue streams. Google had successfully imposed computer mediation on broad new domains of human behaviour as people engaged online. Zuboff explains how these activities were ‘informated’ – a term coined by the author – for the first time and therefore produced whole new data resources. For example, “in addition to keywords, each Google search produces a wake of collateral data such as the number and pattern of search terms, how a query is phrased, spelling, punctuation, dwell times, click patterns and location”. These behavioural by-products are also known as data exhaust, the trail of data left by the activities of an internet user during their online activity, behaviour and transactions. In the early days, these behavioural by-products were haphazardly stored and disregarded for operational purposes. Therefore, in the beginning, search function produced a balance of power – search needed people to learn from and people needed search to learn from. The more search queries, the more relevance, which meant more searches and users. During this period, behavioural data was put entirely to work on the user’s behalf. It was all reinvested in the improvement of the product or service. The important moment for Google came when its engineers, and specifically Ami Patel, realised that these data logs, or the data exhaust, could produce detailed stories about each user, about their thoughts, feelings and interests. The behavioural surplus could, therefore, be used for predictive analysis – the process of being able to deliver a particular message to a particular person at just the moment when it might have a high probability of actually influencing her behaviour is the holy grail of advertising. It was the exploration of possible bigger revenue streams that led to a new rhetoric taking hold of the company. As Zuboff asserts: “Google had discovered a way to translate its non-market interactions with users into surplus material for the fabrication of products aimed at genuine market transactions with its real customers: advertisers”, and this meant “that the raw materials or exhaust would no longer be used to only improve the quality of the search engine, but also be put to use in the service of targeting advertising to individual users”. Behavioural surplus and targeted advertising defined Google’s earning success. By 2002, revenues exploded and the influx of cash never stopped, definitive evidence that behavioural surplus combined with Google’s proprietary analytics were working better than even the engineers anticipated. Revenues went to $347-million in 2002, then $1.5-billion in 2003 and $3.5-billion in 2004. In other words, the discovery of behavioural surplus had produced a stunning 3,590% increase in revenue in less than four years. Google went from serving its customers to surveilling them,  “reinvestment in user service became the method of attracting behavioural surplus and users became the unwitting suppliers of raw material for a larger cycle of revenue generation”. And henceforth revenues and growth would depend upon more behavioural surplus – more data about behaviour means more accurate predictive analysis, means more value to advertisers means more revenue. Behavioural surplus ultimately led to Google arguably becoming the most influential corporation in the history of the world, establishing an architecture marked by “unprecedented concentrations of wealth, knowledge and power”.  1 The result of the discovery of behavioural surplus is that our lives are pervasively rendered as information. We are now thoroughly embedded in this digital architecture. Google’s stores of surplus or exhaust now embrace everything in the online world: “searches, email correspondence, texts, photos, songs, messages, videos, locations, communication patterns, attitudes, preferences, interests, faces, emotions, illnesses, social networks, purchases”, and so on. Of course, other companies followed, including Microsoft and Facebook. (In fact, in 2008, Facebook hired Google executive Sheryl Sandberg as chief operating officer – where she had led the development of surveillance through the expansion of AdWords and other elements of online sales operations). Christopher Wiley, the man who helped set up and then blew the whistle on Cambridge Analytica explains that in the years leading up to the first Obama campaign, a new logic of accumulation was emerging in the boardrooms of Silicon Valley, namely, that people’s identities began to be profiled from their data exhaust or digital breadcrumbs: “More data led to more profits, and so design patterns were implemented to encourage users to share more and more about themselves. Platforms started to mimic casinos, with innovations like the infinite scroll and addictive features aimed at the brain’s reward systems. Services such as Gmail began trawling through our correspondence in a way that would land a traditional postal worker in prison”, and “soon we were sharing personal information without the slightest hesitation. This was encouraged, in part, by a new vocabulary. What were in effect privately owned surveillance networks became ‘communities’, the people these networks used for profits were ‘users’, and addictive design was promoted as ‘user experience’ or ‘engagement’.” And we can, of course, add to this the concept of “personalisation” which, as illustrated by data scientists, represents some of the most aggressive data extraction tactics. Research in top academic journals such as Proceedings of the National Academy of Sciences, Psychological Science and the Journal of Personality and Social Psychology into using social data to infer the psychological disposition of individuals have demonstrated clear evidence that the patterns of a social media user’s likes, status updates, groups, follows and clicks, all serve as clues that could accurately reveal a person’s personality profile when compiled together. Such a profile can then be used to influence users, whether to buy something, to change or cultivate opinion, or to sway a vote. Zuboff illustrates in a detailed excavation of the history and operations of Google and other companies the astonishing speed at which most of these developments took place. Surveillance capitalism took root extraordinarily quickly – as we were celebrating a networked world and the wondrous possibilities that went along with it, new territories of power and knowledge were birthed. While we were in awe of the enriching capabilities and prospects of the digital sphere, others were rapidly claiming ownership of it. The democratisation of knowledge became the means to others’ ends at breakneck speed. The pace of these developments and discoveries also led to the conflation of behavioural surplus operations and predictive analysis with digital technology itself. The Aware Home demonstrates that the digital can take on many forms. The operations that Zuboff describes as “surveillance capitalism” is not digital technology, but rather a specific “logic that imbues technology” and “technologies are always economic means, not ends in themselves”. Google has become masterful at exploiting this conflation. Consider Eric Schmidt’s (CEO of Google 2001-2011) response in 2009 when the public became aware that Google maintains our search histories indefinitely. This was accompanied by the public realisation that data which is available as raw material supplies is also available to intelligence and law-enforcement agencies. Schmidt responded that “the reality is that search engines including Google do retain this information for some time”. This is, of course, false. Search engines do not retain anything, they must be instructed to do so. Google retains this information by design for predictive analysis purposes. Zuboff asserts: “For all Google’s technological prowess and computational talent, the real credit for its success goes to the radical social relations that the company declares as facts, beginning with its disregard for the boundaries of private human experience and the moral integrity of the autonomous individual”. It is, of course, more than possible to personalise or improve a product or service without using, sharing and selling users’ data for the purposes of predictive analysis that serve advertisers, and other unnamed third parties. The internet has become essential for social participation and commerce. In this way, companies like Google and Facebook present us with an illegitimate choice – the price you pay to engage with the dominant means of social interaction of our time and to be commercially competitive, is privacy. We can hardly choose not to be part of this digital architecture. Cameron F. Kerry, former counsel at the U.S. Department of Commerce and visiting fellow at the Brookings Institution, explains that our data universe keeps on expanding and the volume of the information in the world doubles every two years. The idea of informed consent has become ludicrous. It is impractical for people to thoroughly understand and read through contracts that multiply in real-time and that are designed to disguise how surplus data is used. Moreover, the individual choice becomes meaningless as increasingly automated data collection leaves no opportunity for any real notice, much less individual consent. As Kerry asserts, “the fact is that we know very little about how the businesses that collect our data operate and there are enormous disparities of information between the individual and the companies that they deal with”. A 2015 analysis of the top one million websites by the University of Pennsylvania found that 90% of them leaked data to an average of nine external domains that track and expropriate user data for commercial purposes. What is even more astonishing is that 78% initiate third-party transfers to a domain owned by one company: Google; 34% transfer to a Facebook-owned domain. Data scientists and tens of thousands of data researchers in academia and business are constantly discovering new information that can be learned from data about people and new ways in which those businesses use that data. It is nonsensical to expect anyone to thoroughly understand how important data about them is exchanged. Furthermore, as Kerry explains, “we do not get asked for consent to the terms of surveillance cameras on the streets or beacons in stores that pick-up cell phone identifiers, and house guests aren’t generally asked if they agree to homeowners’ smart speakers picking up their speech”. Data scientists have also showed how companies hide facts in rhetoric, omission, complexity, exclusivity, scale, design and euphemisms. Legal experts often refer to the “terms of service agreements” of these companies as “contracts of adhesion”. These contracts impose take it, or leave it conditions that stick to a user whether they like it, or not. For example, Arnold Roosendal, a privacy researcher, demonstrated in 2010 that the Facebook like button installs cookies in users’ computers, whether they click on the button, or not. This then allows Facebook to track users across the internet, and even track non-Facebook members; in other words, all web users. Zuckerberg called Roosendal’s discovery a “bug” and declared that such “missteps” will be attended to. It is when researchers and commentators carefully examine the happenings, events, operations and the types of companies that Facebook and Google invest in, that a clear pattern emerges: The acquiring of more powerful supply mechanisms for behavioural surplus. Apart from ‘take it or leave it’ conditions, it is also standard to include terms of service that can be altered unilaterally by a firm at any time, without user knowledge, or consent. Further, the terms usually implicate other companies (partners, suppliers, marketers, advertising, intermediaries et cetera), without stating, or excepting responsibility for their terms of service. The eco-system is therefore designed in such a way that it doesn’t give users a meaningful option to “opt-out”. Law Professor Nancy Kim has described these standard contracts as “sadistic”. Legal Scholar Margaret Radin points to the unilateral seizure of rights without consent and regards these contracts as a moral and democratic degradation of the rule of law and the institution of the contract. These contracts are designed “to extract from consumers additional benefits unrelated to the transaction”. In 2008, two Carnegie Mellon professors calculated that a reasonable reading of all the privacy policies that one encounters in a year would require 76 full workdays at a national opportunity cost of $781-billion (about R11-trillion). Imagine what those numbers would look like today. Many have celebrated the fall of privacy, heralding it as a new social norm and the fact that companies like Google, and Facebook have the capabilities to infer and deduce the thoughts, feelings, and the intentions of individuals and groups (irrespective of a person’s knowledge or consent) hardly seems to be noteworthy to us anymore. Even as we have become aware of surveillance capitalism’s other uses, behavioural surplus can be used to win elections as illustrated by the Obama campaign in 2008, in which Eric Schmidt played a leading role in implementing data strategies with the science of behavioural prediction, targeting specific voters at specific times with specific messages; and as illustrated by the Cambridge Analytica scandal that saw Facebook basically give away private user information to anyone who might be interested. Facebook and Google also aggressively lobby for campaigns aimed at weakening legislation that seek to regulate biometric data and protect privacy. This involves lobbying the left and the far right in the US, including anti-government groups opposed to regulation and deniers of climate change. Commentators have also pointed to the growing interdependency, especially after 9/11, between Silicon Valley and the US national security agencies, including the Central Intelligence Agency. Zuboff states that if the digital future is to be our home, we must make it so. How exactly remains a question for us to answer. What is certain is that we will have to invent new terms of politics and action. Our voices, personalities and emotions are dissected through apps, online shopping, the live geo-tracking devices we carry everywhere with us and the digital assistants that pick up our speech in our homes. The right to sanctuary is the “human need for a space of inviolable refuge”. Twenty years ago, the violations described above would have been unimaginable to many. Today, one is hard-pressed to figure out a way to invoke protection against the digital forces that besiege our home, that place that Bachelard describes as “the lodging of the soul”. We have yet to fully realise the implications of the loss of the right to sanctuary. Zuboff makes some sense of the implications by recalling historian Karl Polanyi’s 1944 narrative of a self-regulating market economy in three crucial mental inventions he called “commodity fictions”: “The first was that human life could be subordinated to market dynamics and reborn as ‘labour’ to be bought and sold. The second was that nature could be translated into the market and reborn as ‘land’ or ‘real estate’. The third was that exchange could be reborn as ‘money’”. Google’s discovery of the uses of behavioural surplus for the purposes of predictive analyses signals the invention of a fourth commodity fiction. If we thought that at least our feelings, thoughts, emotions, conversations, rituals, habits and our sense of home as a refuge, and as sanctuary could not be possessed in capitalist terms, Zuboff shows us that the surveillance capitalist claiming of the digital realm has made it so. ML Yvonne Jooste is a former senior lecturer in law. She taught at Stellenbosch University and the University of Pretoria where she also obtained her Doctorate in Jurisprudence in 2016. Jooste is currently a freelance academic editor and proofreader, who embarked on a career in freelance writing. She is specifically interested in how technology impacts our lives and the legal implications of dominant digital technologies. She has written on legal tech and the intersection between education and travel and has published a number of articles in academic journals on gender and the law, post-apartheid jurisprudence and legal education. Please note you must be a Maverick Insider to comment. Sign up here or sign in if you are already an Insider.