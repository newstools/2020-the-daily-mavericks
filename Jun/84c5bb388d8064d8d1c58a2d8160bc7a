LONDON, ENGLAND - DECEMBER 02: Some of the photographs of a million users of Facebook collated by artists Paolo Cirio and Alessandro Ludovico are displayed at the Big Bang Data exhibition at Somerset House on December 2, 2015 in London, England. The show highlights the data explosion that's radically transforming our lives. It opens on December 3, 2015 and runs until February 28, 2016 at Somerset House. (Photo by Peter Macdiarmid/Getty Images for Somerset House) On 13 April 2020, a US court in the Southern District of New York ruled against Stephanie Sinclair, a professional photographer who sued the website Mashable for using her image in an article titled, “10 female photojournalists with their lenses on social justice”, published in March 2016. Prior to publishing, a Mashable employee contacted Sinclair to ask if they could license a photograph she took, to use as part of the article. They offered her $50 and she declined. Sinclair had posted the image to her public Instagram account. Mashable went ahead and embedded the Instagram post on their article. About 18 months later, in January 2018, Sinclair asked that they remove the embedded image from the article. Mashable refused. Sinclair sued and eventually lost. What worked against her and in Mashable’s favour was this clause on Instagram’s terms and conditions: “We do not claim ownership of your content that you post on or through the Service. Instead, when you share, post, or upload content that is covered by intellectual property rights (like photos or videos), on or in connection with our Service, you hereby grant to us a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of your content (consistent with your privacy and application settings).” Mashable argued that they had sub-licensed the image from Instagram, and therefore did not need Sinclair’s permission, since she had given Instagram the right to sub-license the content by signing up to the platform and posting on her public profile. She argued that it was unfair to expect her as a professional photographer to set her account to “private mode” on “one of the most popular public photo sharing platforms in the world”. The court agreed that her “dilemma is a real one. But by posting the photograph to her public Instagram account, Plaintiff made her choice. This Court cannot release her from the agreement she made.” As of February 2020, there are now over a 100 million images and videos shared on Instagram every single day, and according to venture capitalist and “Queen of the Internet” Mary Meeker’s 2014 annual internet trends report, on average, back then, we were already uploading some 1.8 billion images every day across the internet. Granted, today many of these would be on semi-private platforms such as WhatsApp and Google Photos. However, even Google Photos, which can automatically upload all images taken on your phone, privately, have some caveats. For example, should you share an image uploaded to Google Photos with another account, a shareable link is formed that is not entirely private. Were someone else to get a hold of the link, they too would be able to view the image, even without a Google account. The link and its secrecy, then becomes the only form of protection for your image. Various platforms have their terms and conditions (Ts&Cs), and according to numerous studies, the overwhelming majority of people do not read them. This US survey by Deloitte found that 91% of people consented to legal terms and services conditions without reading them. And that percentage goes up to 97% in the 18-35 age group. Another study, a collaboration between researchers at the University of Toronto and the University of Connecticut tested how far users could be conned into accepting ridiculous conditions. They created Name Drop, a fake social media platform, and got users to sign up, with ridiculous conditions such as that users give up their first-born child as payment, and that anything they shared would be passed on to the US’s National Security Agency and 98% signed. Considering the amount of Ts&Cs we sign daily on the internet, there is no great surprise there. Depending on one’s view, it could be argued that when it comes to images, the exchange between users and social media giants like Facebook, Instagram and Twitter is an imperfect, but mutually beneficial one. Users get to store and share their images with their “communities”, while social media companies get to monetise their platforms through advertising and use of data, which can be sold to other companies or used to offer advertisers much more targeted ads. The amount of data collected these days has greatly improved some industries, like journalism, for example. As illustrated in this New York Times article, using smartphone data to track the movements of Americans to show how Covid-19 spread across the US in the first quarter of 2020. Access to data has vastly improved our understanding of society, and in many cases, put us in a better position to predict outcomes and propose solutions. Behind the scenes, machine learning programs are learning from our images, and the information we share along with our images, allowing companies to produce the kind of software that has become eerily good at face and object recognition. How this technology might be used in the future is unclear. “When it comes to Facebook and Instagram, for example, where there is so much conversation happening with each photo we share, we tag people, we describe places and locations, we add captions to our images. All that is mineable data which can be used outside the intended context,” says Babusi Nyoni, an innovator, app developer and co-founder of Triple Black agency, who is also working with Google on some projects. He doesn’t think most people’s paranoia about their online images is warranted: “I think those concerns around privacy are not founded if a user just does the bare minimum on social media, just the usual posting and sharing and commenting. I think where they are relevant is when it comes to intellectual property, people posting their designs or photographs and handing over some rights to the companies. An even greater concern would be for companies sharing sensitive information. But for regular everyday use, I don’t think we should worry, that is of course excluding outlier events where things don’t go as planned, such as data breaches.” Some large-scale use of publicly available images has caused some to be concerned, especially when it comes to the rise of face-recognition software. Then little-known Australian tech entrepreneur, Mr Hon Ton-That’s company, Clearview AI, raised a lot of eyebrows in January 2020, when the New York Times published an article about its facial recognition software. According to the mission statement on Clearview’s website: “Clearview AI is a new research tool used by law enforcement agencies to identify perpetrators and victims of crimes. Clearview AI’s technology has helped law enforcement track down hundreds of at-large criminals, including paedophiles, terrorists and sex traffickers. It is also used to help exonerate the innocent and identify the victims of crimes including child sex abuse and financial fraud. Using Clearview AI, law enforcement is able to catch the most dangerous criminals, solve the toughest cold cases and make communities safer, especially the most vulnerable among us.” In a February 2020 interview with CNN Business, Ton-That claimed that his company is working with over 600 law enforcement agencies in the US and Canada. Catching criminals is all good and well, and may sound like a fair trade for relative loss of privacy, however, the controversy when it comes to Clearview’s methods is that the program downloaded over 3 billion publicly available images off the internet, including Facebook, Instagram, Twitter and YouTube. Clearview also keeps the images in its database, even if users eventually go private or delete the images and accounts. As part of the interview with Ton-That, CNN Business tested the technology on camera, first on the host, and then on one of the producers, and both times it worked eerily well, reports CNN Business: “My producer noticed that Clearview had found pictures from her Instagram account, even though her account has been private, accessible only to her followers. Ton-That explained that Clearview had probably downloaded the photos from her account before she had made it private last year.” Mr Ton-That also said a number of banks were using his software, although he declined to name them or give details as to what they were doing with it. Another investigative article in February 2020 from Buzzfeed News, revealed that although Ton-That publicly presents the company as being focused on the US and Canada, the actual client list included “at least 26 countries outside the US, engaging national law enforcement agencies, government bodies, and police forces in Australia, Belgium, Brazil, Canada, Denmark, Finland, France, Ireland, India, Italy, Latvia, Lithuania, Malta, the Netherlands, Norway, Portugal, Serbia, Slovenia, Spain, Sweden, Switzerland, and the United Kingdom,” as well as an expansion plan to include Brazil, Colombia and Nigeria. Although Twitter, Facebook and YouTube have all come out in various ways to demand that Clearview AI cease from using the images on their platforms, Ton-That maintains that they have every right to the publicly available images. However, the company has also said that members of the public can request to have their images removed from the database. The catch is that unless one is signed up to the service and approved by Clearview, there is no way to access their database. Another, more freely available service, is Pimeyes, which allows users to upload only a picture with no other details, and within seconds, the website searches the internet (except social media), to find other images of the person. The website pitches their service as a way for people to find out if their images have been used on the internet without their permission. The free version lets you see the images as well as the names of websites where they feature. The paid version will give you the exact URL of the page. I tried it. Indeed, pictures of me that I didn’t know existed, pitched up on websites that published images of events I had attended. While this could be a good thing, there is also no denying the potential uses for stalkers who could upload images of their potential victims and then find their names. With their names in hand, then they are able to get access to publicly posted social media images and content. What the controversy around Clearview has proven is that as Nyoni says above about our public social media data: “All that is mineable data which can be used outside the intended context.” While we may not need to worry, or while we may have no way to imagine future case use scenarios, the fact is that by 2019, Clearview had already scraped some 3 billion users’ photos from social media, stored them on their database and used them with banks and governments around the world, without the user’s direct permissions or knowledge. In countries like China, where there has been a far greater uptake of facial recognition technology, political dissenters claim to be under constant surveillance from government, and lawbreakers for offences as minor as jaywalking have their faces recorded, and rebroadcast on public screens to name and shame them. Such instances can cause concern about the future uses of the technology, especially if governments and corporations are able to mine image data publicly available on social media. Whether one should be posting images on social media or not, setting to private mode or not, remains a personal decision. However, that decision may not be as easy as it sounds. With some 3.8 billion social media users, there is no denying that access to certain networks and systems is synonymous with access to social media, even more so in the era of social distancing. In exchange for much-needed publicity and access to audiences, for many creators like Sinclair, they will have no choice but to, as Instagram puts it: “grant to us a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of your content.” DM/ML Please note you must be a Maverick Insider to comment. Sign up here or sign in if you are already an Insider.